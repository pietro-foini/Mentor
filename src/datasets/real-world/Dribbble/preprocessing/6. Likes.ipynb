{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import sqlite3\n",
    "\n",
    "from pathlib import Path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to database sqlite where to store the information regarding the Dribbble dataset.\n",
    "conn = sqlite3.connect(\"dribbble_temporary.db\")\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Likes\n",
    "\n",
    "This file contains information about the **likes** related only to the shots (posts). The information regarding the likes on the comments is available in the 'comments' table (it is available only the information regarding the amount of likes received from the comments).\n",
    "\n",
    "The corresponding raw data is the file *likes.tsv*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to open the dataframe in order to visualize it.\n",
    "likes = dd.read_csv(Path(\"raw_data/likes.tsv\"), sep = \"\\t\", header = None, names = [\"shot_id\", \"info\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shot_id</th>\n",
       "      <th>info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>345360</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>587156</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1590969</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1786592</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1217250</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   shot_id info\n",
       "0   345360   []\n",
       "1   587156   []\n",
       "2  1590969   []\n",
       "3  1786592   []\n",
       "4  1217250   []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shot_id</th>\n",
       "      <th>info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6041</th>\n",
       "      <td>4057382</td>\n",
       "      <td>[[106071677, 1514878296, \"zhaoxiangan\"], [1060...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6042</th>\n",
       "      <td>4045712</td>\n",
       "      <td>[[105678380, 1514323145, \"khatib\"], [105651598...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6043</th>\n",
       "      <td>4057420</td>\n",
       "      <td>[[106134902, 1514915239, \"IvanNikolow\"], [1061...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6044</th>\n",
       "      <td>4049411</td>\n",
       "      <td>[[105915954, 1514564126, \"CauMardegan\"], [1058...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6045</th>\n",
       "      <td>4057416</td>\n",
       "      <td>[[106076513, 1514881164, \"_philmillward\"], [10...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      shot_id                                               info\n",
       "6041  4057382  [[106071677, 1514878296, \"zhaoxiangan\"], [1060...\n",
       "6042  4045712  [[105678380, 1514323145, \"khatib\"], [105651598...\n",
       "6043  4057420  [[106134902, 1514915239, \"IvanNikolow\"], [1061...\n",
       "6044  4049411  [[105915954, 1514564126, \"CauMardegan\"], [1058...\n",
       "6045  4057416  [[106076513, 1514881164, \"_philmillward\"], [10..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likes.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2484405"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the number of rows of the dataframe.\n",
    "likes.shape[0].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each shot has a list of lists. Each list contains information about the id of the like, the time (Unix Time) in which the like was left and the username of the user who left the like, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unpack information from raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the likes table into database sqlite.\n",
    "conn.execute(\"CREATE TABLE likes (shot_id INT, like_id INT, created_at_unix INT, author_like TEXT)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file that maps the multiple usernames of the 'users' table into an unique information.\n",
    "with open(\"users_mapper_username.json\") as f:\n",
    "    map_users_multiple_profiles = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the large .tsv file with specified chunksize in order to preprocess it not all together.\n",
    "chunksize = 10000\n",
    "likes_chunk = pd.read_csv(Path(\"raw_data/likes.tsv\"), sep = \"\\t\", names = [\"shot_id\", \"info\"], chunksize = chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative not empty chunk: 30000\n",
      "Cumulative not empty chunk: 40000\n",
      "Cumulative not empty chunk: 50000\n",
      "Cumulative not empty chunk: 60000\n",
      "Cumulative not empty chunk: 70000\n",
      "Cumulative not empty chunk: 80000\n",
      "Cumulative not empty chunk: 90000\n",
      "Cumulative not empty chunk: 100000\n",
      "Cumulative not empty chunk: 110000\n",
      "Cumulative not empty chunk: 120000\n",
      "Cumulative not empty chunk: 130000\n",
      "Cumulative not empty chunk: 140000\n",
      "Cumulative not empty chunk: 150000\n",
      "Cumulative not empty chunk: 160000\n",
      "Cumulative not empty chunk: 170000\n",
      "Cumulative not empty chunk: 180000\n",
      "Cumulative not empty chunk: 190000\n",
      "Cumulative not empty chunk: 200000\n",
      "Cumulative not empty chunk: 210000\n",
      "Cumulative not empty chunk: 220000\n",
      "Cumulative not empty chunk: 230000\n",
      "Cumulative not empty chunk: 240000\n",
      "Cumulative not empty chunk: 250000\n",
      "Cumulative not empty chunk: 260000\n",
      "Cumulative not empty chunk: 270000\n",
      "Cumulative not empty chunk: 280000\n",
      "Cumulative not empty chunk: 290000\n",
      "Cumulative not empty chunk: 300000\n",
      "Cumulative not empty chunk: 310000\n",
      "Cumulative not empty chunk: 320000\n",
      "Cumulative not empty chunk: 330000\n",
      "Cumulative not empty chunk: 340000\n",
      "Cumulative not empty chunk: 350000\n",
      "Cumulative not empty chunk: 360000\n",
      "Cumulative not empty chunk: 370000\n",
      "Cumulative not empty chunk: 380000\n",
      "Cumulative not empty chunk: 390000\n",
      "Cumulative not empty chunk: 400000\n",
      "Cumulative not empty chunk: 410000\n",
      "Cumulative not empty chunk: 420000\n",
      "Cumulative not empty chunk: 430000\n",
      "Cumulative not empty chunk: 440000\n",
      "Cumulative not empty chunk: 450000\n",
      "Cumulative not empty chunk: 460000\n",
      "Cumulative not empty chunk: 470000\n",
      "Cumulative not empty chunk: 480000\n",
      "Cumulative not empty chunk: 490000\n",
      "Cumulative not empty chunk: 500000\n",
      "Cumulative not empty chunk: 510000\n",
      "Cumulative not empty chunk: 520000\n",
      "Cumulative not empty chunk: 530000\n",
      "Cumulative not empty chunk: 540000\n",
      "Cumulative not empty chunk: 550000\n",
      "Cumulative not empty chunk: 560000\n",
      "Cumulative not empty chunk: 570000\n",
      "Cumulative not empty chunk: 580000\n",
      "Cumulative not empty chunk: 590000\n",
      "Cumulative not empty chunk: 600000\n",
      "Cumulative not empty chunk: 610000\n",
      "Cumulative not empty chunk: 620000\n",
      "Cumulative not empty chunk: 630000\n",
      "Cumulative not empty chunk: 640000\n",
      "Cumulative not empty chunk: 650000\n",
      "Cumulative not empty chunk: 660000\n",
      "Cumulative not empty chunk: 670000\n",
      "Cumulative not empty chunk: 680000\n",
      "Cumulative not empty chunk: 690000\n",
      "Cumulative not empty chunk: 700000\n",
      "Cumulative not empty chunk: 710000\n",
      "Cumulative not empty chunk: 720000\n",
      "Cumulative not empty chunk: 730000\n",
      "Cumulative not empty chunk: 740000\n",
      "Cumulative not empty chunk: 750000\n",
      "Cumulative not empty chunk: 760000\n",
      "Cumulative not empty chunk: 770000\n",
      "Cumulative not empty chunk: 780000\n",
      "Cumulative not empty chunk: 790000\n",
      "Cumulative not empty chunk: 800000\n",
      "Cumulative not empty chunk: 810000\n",
      "Cumulative not empty chunk: 820000\n",
      "Cumulative not empty chunk: 830000\n",
      "Cumulative not empty chunk: 840000\n",
      "Cumulative not empty chunk: 850000\n",
      "Cumulative not empty chunk: 860000\n",
      "Cumulative not empty chunk: 870000\n",
      "Cumulative not empty chunk: 880000\n",
      "Cumulative not empty chunk: 890000\n",
      "Cumulative not empty chunk: 900000\n",
      "Cumulative not empty chunk: 910000\n",
      "Cumulative not empty chunk: 920000\n",
      "Cumulative not empty chunk: 930000\n",
      "Cumulative not empty chunk: 940000\n",
      "Cumulative not empty chunk: 950000\n",
      "Cumulative not empty chunk: 960000\n",
      "Cumulative not empty chunk: 970000\n",
      "Cumulative not empty chunk: 980000\n",
      "Cumulative not empty chunk: 990000\n",
      "Cumulative not empty chunk: 1000000\n",
      "Cumulative not empty chunk: 1010000\n",
      "Cumulative not empty chunk: 1020000\n",
      "Cumulative not empty chunk: 1030000\n",
      "Cumulative not empty chunk: 1040000\n",
      "Cumulative not empty chunk: 1050000\n",
      "Cumulative not empty chunk: 1060000\n",
      "Cumulative not empty chunk: 1070000\n",
      "Cumulative not empty chunk: 1080000\n",
      "Cumulative not empty chunk: 1090000\n",
      "Cumulative not empty chunk: 1100000\n",
      "Cumulative not empty chunk: 1110000\n",
      "Cumulative not empty chunk: 1120000\n",
      "Cumulative not empty chunk: 1130000\n",
      "Cumulative not empty chunk: 1140000\n",
      "Cumulative not empty chunk: 1150000\n",
      "Cumulative not empty chunk: 1160000\n",
      "Cumulative not empty chunk: 1170000\n",
      "Cumulative not empty chunk: 1180000\n",
      "Cumulative not empty chunk: 1190000\n",
      "Cumulative not empty chunk: 1200000\n",
      "Cumulative not empty chunk: 1210000\n",
      "Cumulative not empty chunk: 1220000\n",
      "Cumulative not empty chunk: 1230000\n",
      "Cumulative not empty chunk: 1240000\n",
      "Cumulative not empty chunk: 1250000\n",
      "Cumulative not empty chunk: 1260000\n",
      "Cumulative not empty chunk: 1270000\n",
      "Cumulative not empty chunk: 1280000\n",
      "Cumulative not empty chunk: 1290000\n",
      "Cumulative not empty chunk: 1300000\n",
      "Cumulative not empty chunk: 1310000\n",
      "Cumulative not empty chunk: 1320000\n",
      "Cumulative not empty chunk: 1330000\n",
      "Cumulative not empty chunk: 1340000\n",
      "Cumulative not empty chunk: 1350000\n",
      "Cumulative not empty chunk: 1360000\n",
      "Cumulative not empty chunk: 1370000\n",
      "Cumulative not empty chunk: 1380000\n",
      "Cumulative not empty chunk: 1390000\n",
      "Cumulative not empty chunk: 1400000\n",
      "Cumulative not empty chunk: 1410000\n",
      "Cumulative not empty chunk: 1420000\n",
      "Cumulative not empty chunk: 1430000\n",
      "Cumulative not empty chunk: 1440000\n",
      "Cumulative not empty chunk: 1450000\n",
      "Cumulative not empty chunk: 1460000\n",
      "Cumulative not empty chunk: 1470000\n",
      "Cumulative not empty chunk: 1480000\n",
      "Cumulative not empty chunk: 1490000\n",
      "Cumulative not empty chunk: 1500000\n",
      "Cumulative not empty chunk: 1510000\n",
      "Cumulative not empty chunk: 1520000\n",
      "Cumulative not empty chunk: 1530000\n",
      "Cumulative not empty chunk: 1540000\n",
      "Cumulative not empty chunk: 1550000\n",
      "Cumulative not empty chunk: 1560000\n",
      "Cumulative not empty chunk: 1570000\n",
      "Cumulative not empty chunk: 1580000\n",
      "Cumulative not empty chunk: 1590000\n",
      "Cumulative not empty chunk: 1600000\n",
      "Cumulative not empty chunk: 1610000\n",
      "Cumulative not empty chunk: 1620000\n",
      "Cumulative not empty chunk: 1630000\n",
      "Cumulative not empty chunk: 1640000\n",
      "Cumulative not empty chunk: 1650000\n",
      "Cumulative not empty chunk: 1660000\n",
      "Cumulative not empty chunk: 1670000\n",
      "Cumulative not empty chunk: 1680000\n",
      "Cumulative not empty chunk: 1690000\n",
      "Cumulative not empty chunk: 1700000\n",
      "Cumulative not empty chunk: 1710000\n",
      "Cumulative not empty chunk: 1720000\n",
      "Cumulative not empty chunk: 1730000\n",
      "Cumulative not empty chunk: 1740000\n",
      "Cumulative not empty chunk: 1750000\n",
      "Cumulative not empty chunk: 1760000\n",
      "Cumulative not empty chunk: 1770000\n",
      "Cumulative not empty chunk: 1780000\n",
      "Cumulative not empty chunk: 1790000\n",
      "Cumulative not empty chunk: 1800000\n",
      "Cumulative not empty chunk: 1810000\n",
      "Cumulative not empty chunk: 1820000\n",
      "Cumulative not empty chunk: 1830000\n",
      "Cumulative not empty chunk: 1840000\n",
      "Cumulative not empty chunk: 1850000\n",
      "Cumulative not empty chunk: 1860000\n",
      "Cumulative not empty chunk: 1870000\n",
      "Cumulative not empty chunk: 1880000\n",
      "Cumulative not empty chunk: 1890000\n",
      "Cumulative not empty chunk: 1900000\n",
      "Cumulative not empty chunk: 1910000\n",
      "Cumulative not empty chunk: 1920000\n",
      "Cumulative not empty chunk: 1930000\n",
      "Cumulative not empty chunk: 1940000\n",
      "Cumulative not empty chunk: 1950000\n",
      "Cumulative not empty chunk: 1960000\n",
      "Cumulative not empty chunk: 1970000\n",
      "Cumulative not empty chunk: 1980000\n",
      "Cumulative not empty chunk: 1990000\n",
      "Cumulative not empty chunk: 2000000\n",
      "Cumulative not empty chunk: 2010000\n",
      "Cumulative not empty chunk: 2020000\n",
      "Cumulative not empty chunk: 2030000\n",
      "Cumulative not empty chunk: 2040000\n",
      "Cumulative not empty chunk: 2050000\n",
      "Cumulative not empty chunk: 2060000\n",
      "Cumulative not empty chunk: 2070000\n",
      "Cumulative not empty chunk: 2080000\n",
      "Cumulative not empty chunk: 2090000\n",
      "Cumulative not empty chunk: 2100000\n",
      "Cumulative not empty chunk: 2110000\n",
      "Cumulative not empty chunk: 2120000\n",
      "Cumulative not empty chunk: 2130000\n",
      "Cumulative not empty chunk: 2140000\n",
      "Cumulative not empty chunk: 2150000\n",
      "Cumulative not empty chunk: 2160000\n",
      "Cumulative not empty chunk: 2170000\n",
      "Cumulative not empty chunk: 2180000\n",
      "Cumulative not empty chunk: 2190000\n",
      "Cumulative not empty chunk: 2200000\n",
      "Cumulative not empty chunk: 2210000\n",
      "Cumulative not empty chunk: 2220000\n",
      "Cumulative not empty chunk: 2230000\n",
      "Cumulative not empty chunk: 2240000\n",
      "Cumulative not empty chunk: 2250000\n",
      "Cumulative not empty chunk: 2260000\n",
      "Cumulative not empty chunk: 2270000\n",
      "Cumulative not empty chunk: 2280000\n",
      "Cumulative not empty chunk: 2290000\n",
      "Cumulative not empty chunk: 2300000\n",
      "Cumulative not empty chunk: 2310000\n",
      "Cumulative not empty chunk: 2320000\n",
      "Cumulative not empty chunk: 2330000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative not empty chunk: 2340000\n",
      "Cumulative not empty chunk: 2350000\n",
      "Cumulative not empty chunk: 2360000\n",
      "Cumulative not empty chunk: 2370000\n",
      "Cumulative not empty chunk: 2380000\n",
      "Cumulative not empty chunk: 2390000\n",
      "Cumulative not empty chunk: 2400000\n",
      "Cumulative not empty chunk: 2410000\n",
      "Cumulative not empty chunk: 2420000\n",
      "Cumulative not empty chunk: 2430000\n",
      "Cumulative not empty chunk: 2440000\n",
      "Cumulative not empty chunk: 2450000\n",
      "Cumulative not empty chunk: 2460000\n",
      "Cumulative not empty chunk: 2470000\n",
      "Cumulative not empty chunk: 2480000\n",
      "Cumulative not empty chunk: 2490000\n"
     ]
    }
   ],
   "source": [
    "shots_no_likes = list()\n",
    "\n",
    "# Each chunk is a dataframe.\n",
    "for i,chunk in enumerate(likes_chunk):\n",
    "    # Store information about shots with no likes. \n",
    "    shots_no_likes.append(chunk.loc[(chunk[\"info\"] == \"[]\")][\"shot_id\"].values)\n",
    "    # Remove empty 'info' rows.\n",
    "    chunk = chunk.loc[~(chunk[\"info\"] == \"[]\")]\n",
    "    if not chunk.empty:\n",
    "        print(\"Cumulative not empty chunk: %d\" % (chunksize*(i+1)))\n",
    "        # Read parse a string as a list of lists.\n",
    "        chunk[\"info\"] = chunk[\"info\"].apply(lambda x: json.loads(x))\n",
    "        # Flatten list of list over columns.\n",
    "        chunk = chunk.explode(\"info\")\n",
    "        # We get the all the data information into list of lists (shot_id, like_id, created_at_unix, author_like).\n",
    "        data_to_insert = chunk.shot_id.map(lambda x: [x]) + chunk[\"info\"]\n",
    "        data_to_insert = np.array(data_to_insert.tolist(), dtype = object)\n",
    "\n",
    "        # We map the usernames with multiple profiles into 'users' table to have a single value.\n",
    "        def map_func(val, dictionary):\n",
    "            return dictionary[val] if val in dictionary else val \n",
    "        vfunc  = np.vectorize(map_func)\n",
    "        data_to_insert[:, 3] = vfunc(data_to_insert[:, 3], map_users_multiple_profiles)\n",
    "\n",
    "        # Insert data into sql table recursively.\n",
    "        c.executemany(\"INSERT INTO likes (shot_id, like_id, created_at_unix, author_like) VALUES (?, ?, ?, ?)\", data_to_insert)\n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten list of shots with no likes.\n",
    "shots_no_likes = pd.Series(shots_no_likes).explode().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if there are some NaN.\n",
    "shots_no_likes.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "shots_no_likes.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shots_no_likes.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "shots_no_likes = shots_no_likes.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23874"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shots_no_likes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "likes = pd.read_sql(\"SELECT shot_id FROM likes\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if there exist an intersection between the shot IDs with at least one like and the shot IDs with no likes.\n",
    "list(set(likes.shot_id.unique()) & set(shots_no_likes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicated rows (slow statement).\n",
    "c.execute(\"\"\"DELETE FROM likes WHERE rowid NOT IN (SELECT max(rowid) FROM likes GROUP BY like_id)\"\"\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the right datetime format from Unix Time to UTC creating a new column.\n",
    "c.execute(\"\"\"ALTER TABLE likes ADD COLUMN created_at TIMESTAMP\"\"\")\n",
    "c.execute(\"\"\"UPDATE likes SET created_at = created_at_unix\"\"\")\n",
    "c.execute(\"\"\"UPDATE likes SET created_at = datetime(created_at_unix, 'unixepoch')\"\"\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Inverse operation.\n",
    "c_new.execute(\"\"\"UPDATE likes SET created_at_unix = strftime('%s', created_at)\"\"\")\n",
    "conn_new.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the large file with specified chunksize in order to simply fast visualize it.\n",
    "chunksize = 10000\n",
    "likes_chunk = pd.read_sql(\"SELECT * FROM likes\", conn, chunksize = chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shot_id</th>\n",
       "      <th>like_id</th>\n",
       "      <th>created_at_unix</th>\n",
       "      <th>author_like</th>\n",
       "      <th>created_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2686725</td>\n",
       "      <td>100555000</td>\n",
       "      <td>1509635279</td>\n",
       "      <td>sexysev</td>\n",
       "      <td>2017-11-02 15:07:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2686725</td>\n",
       "      <td>99914321</td>\n",
       "      <td>1509022719</td>\n",
       "      <td>KseniaProkopova</td>\n",
       "      <td>2017-10-26 12:58:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2686725</td>\n",
       "      <td>97342964</td>\n",
       "      <td>1506537160</td>\n",
       "      <td>citrusbyte</td>\n",
       "      <td>2017-09-27 18:32:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2686725</td>\n",
       "      <td>94318870</td>\n",
       "      <td>1503522249</td>\n",
       "      <td>zapadenko</td>\n",
       "      <td>2017-08-23 21:04:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2686725</td>\n",
       "      <td>73483218</td>\n",
       "      <td>1481441298</td>\n",
       "      <td>xt0rted</td>\n",
       "      <td>2016-12-11 07:28:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>3246627</td>\n",
       "      <td>82762150</td>\n",
       "      <td>1491629036</td>\n",
       "      <td>divanraj</td>\n",
       "      <td>2017-04-08 05:23:56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>3246627</td>\n",
       "      <td>82652073</td>\n",
       "      <td>1491495650</td>\n",
       "      <td>brycejacobson</td>\n",
       "      <td>2017-04-06 16:20:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>3246627</td>\n",
       "      <td>82645905</td>\n",
       "      <td>1491490544</td>\n",
       "      <td>florencechevalier</td>\n",
       "      <td>2017-04-06 14:55:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>3246627</td>\n",
       "      <td>81909285</td>\n",
       "      <td>1490728814</td>\n",
       "      <td>krasotin</td>\n",
       "      <td>2017-03-28 19:20:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>3246627</td>\n",
       "      <td>81616393</td>\n",
       "      <td>1490384068</td>\n",
       "      <td>basovdesign</td>\n",
       "      <td>2017-03-24 19:34:28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      shot_id    like_id  created_at_unix        author_like  \\\n",
       "0     2686725  100555000       1509635279            sexysev   \n",
       "1     2686725   99914321       1509022719    KseniaProkopova   \n",
       "2     2686725   97342964       1506537160         citrusbyte   \n",
       "3     2686725   94318870       1503522249          zapadenko   \n",
       "4     2686725   73483218       1481441298            xt0rted   \n",
       "...       ...        ...              ...                ...   \n",
       "9995  3246627   82762150       1491629036           divanraj   \n",
       "9996  3246627   82652073       1491495650      brycejacobson   \n",
       "9997  3246627   82645905       1491490544  florencechevalier   \n",
       "9998  3246627   81909285       1490728814           krasotin   \n",
       "9999  3246627   81616393       1490384068        basovdesign   \n",
       "\n",
       "               created_at  \n",
       "0     2017-11-02 15:07:59  \n",
       "1     2017-10-26 12:58:39  \n",
       "2     2017-09-27 18:32:40  \n",
       "3     2017-08-23 21:04:09  \n",
       "4     2016-12-11 07:28:18  \n",
       "...                   ...  \n",
       "9995  2017-04-08 05:23:56  \n",
       "9996  2017-04-06 16:20:50  \n",
       "9997  2017-04-06 14:55:44  \n",
       "9998  2017-03-28 19:20:14  \n",
       "9999  2017-03-24 19:34:28  \n",
       "\n",
       "[10000 rows x 5 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(likes_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to the dataframe the information regarding the shot IDs with no likes.\n",
    "no_likes = pd.DataFrame(columns = next(likes_chunk).columns)\n",
    "no_likes[\"shot_id\"] = shots_no_likes\n",
    "\n",
    "data_to_insert = no_likes.values.tolist()\n",
    "\n",
    "# Insert data into sql table recursively.\n",
    "c.executemany(\"INSERT INTO likes (shot_id, like_id, created_at_unix, author_like, created_at) VALUES (?, ?, ?, ?, ?)\", data_to_insert)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "likes_chunk.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shot_id</th>\n",
       "      <th>like_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2686725</td>\n",
       "      <td>100555000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2686725</td>\n",
       "      <td>99914321.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2686725</td>\n",
       "      <td>97342964.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2686725</td>\n",
       "      <td>94318870.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2686725</td>\n",
       "      <td>73483218.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   shot_id      like_id\n",
       "0  2686725  100555000.0\n",
       "1  2686725   99914321.0\n",
       "2  2686725   97342964.0\n",
       "3  2686725   94318870.0\n",
       "4  2686725   73483218.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read 'likes' table.\n",
    "likes = pd.read_sql(\"SELECT shot_id, like_id FROM likes\", conn)\n",
    "likes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likes.set_index([\"shot_id\", \"like_id\"]).index.is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.execute(\"VACUUM\"); # This command allows to reaggange database on small data size especially if you have carried out deletions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
